
#FlexiMart ETL Pipeline
#Responsibilities:
#- Clean customers, products, sales CSV
#- Load customers, products, orders tables
#NOTE:
#- order_items loading is handled separately

# ============================================
# FLEXIMART ETL PIPELINE
# STEP 1: EXTRACT CUSTOMERS DATA
# ============================================

import pandas as pd

import mysql.connector
import os

BASE_DIR = os.path.dirname(os.path.dirname(__file__))
DATA_DIR = os.path.join(BASE_DIR, "data")

# MYSQL CONNECTION

db_conn = mysql.connector.connect(
    host="localhost",
    user="root",
    password="sid@2611",  
    database="fleximart"
)

cursor = db_conn.cursor()
print(" MySQL connection successful")

# CSV file ka path
customers_csv_path = os.path.join(DATA_DIR, "customers_raw.csv")

# CSV ko read karna
customers_df = pd.read_csv(customers_csv_path)

# Data ka preview
print("Customers Raw Data (Top 5 rows):")
print(customers_df.head())

# Total number of records
print("\nTotal number of records:", len(customers_df))

# TRANSFORM STEP : Remove garbage rows

# customer_id null wali rows remove
customers_df = customers_df[customers_df['customer_id'].notna()]

print("\nAfter removing garbage rows:")
print("Records:", len(customers_df))

# TRANSFORM STEP : Phone number as string

customers_df['phone'] = customers_df['phone'].astype(str)

print("\nPhone column converted to string")
print(customers_df['phone'].head())

# TRANSFORM STEP : Clean & standardize phone numbers
import re

customers_df['phone'] = pd.to_numeric(customers_df['phone'], errors='coerce')
customers_df['phone'] = customers_df['phone'].astype(str)

def standardize_phone(phone):
    digits = re.sub(r'[^0-9]', '', phone)
    if len(digits) >= 10:
        return "'+91" + digits[-10:]
    else:
        return None

customers_df['phone'] = customers_df['phone'].apply(standardize_phone)


print("\nPhone numbers after standardization:")
print(customers_df['phone'].head())

# TRANSFORM STEP : Handle missing emails

def generate_email(row):
    if pd.isna(row['email']) or row['email'].strip() == '':
        return f"{row['first_name']}.{row['last_name']}.{row['customer_id']}@fleximart.com".lower()
    else:
        return row['email'].strip().lower()

customers_df['email'] = customers_df.apply(generate_email, axis=1)

print("\nEmails after handling missing values:")
print(customers_df[['customer_id', 'email']].head(10))

print("\nCustomer IDs sample:")
print(customers_df['customer_id'].head(10))

print("\nUnique customer_id count:", customers_df['customer_id'].nunique())
print("Total rows:", len(customers_df))

print("\nPhone format check:")
print(customers_df['phone'].unique()[:5])

print("\nMissing emails count:", customers_df['email'].isna().sum())
print("Duplicate emails count:", customers_df['email'].duplicated().sum())

# FINAL TRANSFORM: Remove duplicate customers by customer_id

before = len(customers_df)

customers_df = customers_df.drop_duplicates(subset='customer_id', keep='first')

after = len(customers_df)

print("\nFinal duplicate customers removed:", before - after)
print("Final customer records:", after)

print("\nFINAL CHECK")
print("Total rows:", len(customers_df))
print("Unique customer_id:", customers_df['customer_id'].nunique())
print("Duplicate emails:", customers_df['email'].duplicated().sum())

# TRANSFORM STEP: Robust registration_date cleaning

def clean_registration_date(date_value):
    try:
        parsed_date = pd.to_datetime(date_value, dayfirst=True)
        return parsed_date.strftime('%Y-%m-%d')
    except:
        return '2000-01-01'

customers_df['registration_date'] = customers_df['registration_date'].astype(str)

customers_df['registration_date'] = customers_df['registration_date'].apply(
    clean_registration_date
)

# Final verification
print("\nInvalid registration dates after fix:",
      customers_df['registration_date'].isna().sum())

print("\nRegistration dates after final cleaning:")
print(customers_df['registration_date'].head())

# FINAL TRANSFORM: Clean city names

customers_df['city'] = (
    customers_df['city']
    .astype(str)
    .str.strip()
    .str.title()
)

# Final city verification
print("\nUnique cities after final cleaning:")
print(sorted(customers_df['city'].unique()))

# SAVE CLEAN CUSTOMERS CSV

customers_clean_path = os.path.join(DATA_DIR, "customers_clean.csv")
customers_df.to_csv(customers_clean_path, index=False)

print(f"\n Clean customers data saved to {customers_clean_path}")

# PRODUCTS ETL - STEP : EXTRACT

products_csv_path = os.path.join(DATA_DIR, "products_raw.csv")

products_df = pd.read_csv(products_csv_path)

print("\n================ PRODUCTS RAW DATA ================\n")
print(products_df.head())
print("\nTotal product records:", len(products_df))

# PRODUCTS TRANSFORM - STEP : Clean category names

products_df['category'] = (
   products_df['category']
   .astype(str)
   .str.strip()
   .str.lower()
)

category_mapping = {
   'electronics': 'Electronics',
   'fashion': 'Fashion',
   'groceries': 'Groceries'
}

products_df['category'] = products_df['category'].map(category_mapping)

print("\nUnique categories after cleaning:")
print(products_df['category'].unique())

# PRODUCTS TRANSFORM - STEP : Clean product names

products_df['product_name'] = (
   products_df['product_name']
   .astype(str)
   .str.strip()
)

print("\nSample cleaned product names:")
print(products_df['product_name'].head())

# PRODUCTS TRANSFORM - STEP : Handle missing prices

# Convert price to numeric
products_df['price'] = pd.to_numeric(products_df['price'], errors='coerce')

# Fill missing prices using category median
products_df['price'] = products_df.groupby('category')['price']\
   .transform(lambda x: x.fillna(x.median()))

print("\nMissing prices after fix:", products_df['price'].isna().sum())

# PRODUCTS TRANSFORM - STEP : Handle missing stock

products_df['stock_quantity'] = pd.to_numeric(
   products_df['stock_quantity'], errors='coerce'
).fillna(0).astype(int)

print("\nMissing stock after fix:", products_df['stock_quantity'].isna().sum())

# SAVE CLEAN PRODUCTS CSV

products_clean_path  = os.path.join(DATA_DIR, "products_clean.csv")
products_df.to_csv(products_clean_path, index=False)

print(f"\n Clean products data saved to {products_clean_path}")

# SALES ETL - STEP : EXTRACT

sales_csv_path    = os.path.join(DATA_DIR, "sales_raw.csv")
sales_df = pd.read_csv(sales_csv_path)

print("\n================ SALES RAW DATA ================\n")
print(sales_df.head())
print("\nTotal sales records:", len(sales_df))

# DEBUG: Check extra / garbage rows in sales

print("\nSales DataFrame info:")
print(sales_df.info())

print("\nLast 5 rows of sales data:")
print(sales_df.tail())

# SALES TRANSFORM: Remove garbage rows

before = len(sales_df)

sales_df = sales_df[sales_df['transaction_id'].notna()]

after = len(sales_df)

print("\nGarbage sales rows removed:", before - after)
print("Sales records after garbage removal:", after)

# SALES TRANSFORM: Remove duplicate transactions

before = len(sales_df)

sales_df = sales_df.drop_duplicates(subset='transaction_id', keep='first')

after = len(sales_df)

print("\nDuplicate transactions removed:", before - after)
print("Sales records after deduplication:", after)

# SALES TRANSFORM: Drop rows with missing customer_id

before = len(sales_df)

sales_df = sales_df[sales_df['customer_id'].notna()]

after = len(sales_df)

print("\nRows dropped due to missing customer_id:", before - after)
print("Sales records after customer_id cleaning:", after)

# SALES TRANSFORM: Drop rows with missing product_id

before = len(sales_df)

sales_df = sales_df[sales_df['product_id'].notna()]

after = len(sales_df)

print("\nRows dropped due to missing product_id:", before - after)
print("Sales records after product_id cleaning:", after)

# SALES TRANSFORM: Clean transaction_date

def clean_transaction_date(date_value):
   try:
       parsed_date = pd.to_datetime(date_value, dayfirst=True)
       return parsed_date.strftime('%Y-%m-%d')
   except:
       return None

sales_df['transaction_date'] = sales_df['transaction_date'].apply(clean_transaction_date)

print("\nInvalid transaction dates after cleaning:",
     sales_df['transaction_date'].isna().sum())

# STRICT FK CLEANING: Remove empty strings also

sales_df['customer_id'] = sales_df['customer_id'].astype(str).str.strip()
sales_df['product_id'] = sales_df['product_id'].astype(str).str.strip()

before = len(sales_df)

sales_df = sales_df[
   (sales_df['customer_id'] != '') &
   (sales_df['product_id'] != '')
]

after = len(sales_df)

print("\nRows dropped due to empty customer_id/product_id:", before - after)
print("Sales records after strict FK cleaning:", after)

# SALES TRANSFORM: Add total_amount column

sales_df['total_amount'] = sales_df['quantity'] * sales_df['unit_price']

print("\nTotal amount column added.")
print(sales_df[['transaction_id', 'quantity', 'unit_price', 'total_amount']].head())

print("\nNULL check including total_amount:")
print(
    sales_df[
        ['transaction_id','customer_id','product_id',
         'quantity','unit_price','total_amount','transaction_date']
    ].isna().sum()
)

print("\nTotal amount sanity (min, max):")
print(sales_df['total_amount'].min(), sales_df['total_amount'].max())

# SAVE CLEAN SALES CSV

sales_clean_path     = os.path.join(DATA_DIR, "sales_clean.csv")
sales_df.to_csv(sales_clean_path, index=False)

print(f"\n Clean sales data saved to {sales_clean_path}")

# ====================================================
# LOAD PHASE: INSERT CUSTOMERS
# =====================================================

#insert_customer_sql = """
#INSERT INTO customers
#(first_name, last_name, email, phone, city, registration_date)
#VALUES (%s, %s, %s, %s, %s, %s)
#"""

#customer_records = customers_df[
#    ['first_name', 'last_name', 'email', 'phone', 'city', 'registration_date']
#].values.tolist()

#cursor.executemany(insert_customer_sql, customer_records)
#db_conn.commit()

#print(f" Customers inserted: {cursor.rowcount}")

# =====================================================
# LOAD PHASE: INSERT PRODUCTS
# =====================================================

#insert_product_sql = """
#INSERT INTO products
#(product_name, category, price, stock_quantity)
#VALUES (%s, %s, %s, %s)
#"""

#product_records = products_df[
#    ['product_name', 'category', 'price', 'stock_quantity']
#].values.tolist()

#cursor.executemany(insert_product_sql, product_records)
#db_conn.commit()

#print(f" Products inserted: {cursor.rowcount}")

# =====================================================
# ID MAPPING (CSV → DATABASE IDs)
# =====================================================

# ---- Customer mapping (email → customer_id) ----
# cursor.execute("SELECT customer_id, email FROM customers")
# customer_map = {
#     row[1]: row[0] for row in cursor.fetchall()
# }

# ---- Product mapping (product_id → product_id) ----
# cursor.execute("SELECT product_id FROM products")
# product_map = {
#     row[0]: row[0] for row in cursor.fetchall()
# }


# print(" ID mapping created (customers & products)")

# =====================================================
# CSV CUSTOMER_ID → EMAIL LOOKUP
# =====================================================

# customer_lookup = {}

# for _, row in customers_df.iterrows():
#     customer_lookup[row['customer_id']] = row['email']

# print(" Customer lookup created")

# ============================================
# LOAD PHASE: INSERT ORDERS (CLEAN + SAFE)
# ============================================

#insert_order_sql = """
#INSERT INTO orders
#(transaction_id, customer_id, order_date, total_amount, status)
#VALUES (%s, %s, %s, %s, %s)
#"""

#inserted = 0
#skipped = 0

#for _, row in sales_df.iterrows():

    # CSV customer_id → email → DB customer_id
#    email = customer_lookup.get(row['customer_id'])
#    db_customer_id = customer_map.get(email)

#    if db_customer_id is None:
#        skipped += 1
#        continue

#    cursor.execute(
#        insert_order_sql,
#        (
#            row['transaction_id'],
#            db_customer_id,
#            row['transaction_date'],
#            row['total_amount'],
#            row['status']
#        )
#    )
#    inserted += 1

#db_conn.commit()

#print(f" Orders inserted: {inserted}")
#print(f" Orders skipped: {skipped}")

# =====================================================
# BUILD ORDER_ID_MAP FROM ORDERS (FINAL & SAFE)
# =====================================================

# cursor.execute("""
#     SELECT order_id, transaction_id
#     FROM orders
# """)

# order_id_map = {}

# for order_id, transaction_id in cursor.fetchall():
#     order_id_map[str(transaction_id)] = order_id

# print(" order_id_map ready:", len(order_id_map))

cursor.close()
db_conn.close()
print(" Database connection closed") 